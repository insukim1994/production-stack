{{- if and .Values.servingEngineSpec.enableEngine .Values.servingEngineSpec.enableKubeRay }}
{{- range $modelSpec := .Values.servingEngineSpec.modelSpec }}
{{- with $ -}}
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: "{{ .Release.Name }}-{{$modelSpec.name}}-raycluster-vllm"
  namespace: {{ .Release.Namespace }}
  labels:
    helm-release-name: {{ .Release.Name }}
spec:
  headGroupSpec:
    serviceType: ClusterIP
    rayStartParams:
      dashboard-host: "0.0.0.0"
    template:
      metadata:
        labels:
          model: {{ $modelSpec.name }}
          {{- include "chart.engineLabels" . | nindent 10 }}
      spec:
        terminationGracePeriodSeconds: 0
        {{- if .Values.servingEngineSpec.securityContext }}
        securityContext:
          {{- toYaml .Values.servingEngineSpec.securityContext | nindent 10 }}
        {{- end }}
        containers:
          - name: vllm-ray-head
            image: "{{ required "Required value 'modelSpec.repository' must be defined !" $modelSpec.repository }}:{{ required "Required value 'modelSpec.tag' must be defined !" $modelSpec.tag }}"
            livenessProbe:
              exec:
                command: ["/bin/bash", "-c", "echo TBD"]
            readinessProbe:
              exec:
                command: ["/bin/bash", "-c", "echo TBD"]
            env:
              - name: EXPECTED_NODES
                value: "{{ add $modelSpec.replicaCount 1}}"
            startupProbe:
              exec:
                command: ["/bin/bash", "-c", "python3 /scripts/wait_for_ray.py"]
              failureThreshold: 30
              periodSeconds: 15
              timeoutSeconds: 10
            volumeMounts:
              - name: wait-script
                mountPath: /scripts
                readOnly: true
        volumes:
          - name: wait-script
            configMap:
              name: wait-for-ray-script
  workerGroupSpecs:
    - rayStartParams: {}
      replicas: {{ $modelSpec.replicaCount }}
      groupName: vllm-ray-worker
      template:
        {{- if .Values.servingEngineSpec.securityContext }}
        securityContext:
          {{- toYaml .Values.servingEngineSpec.securityContext | nindent 8 }}
        {{- end }}
        spec:
          containers:
            - name: vllm-ray-worker
              image: "{{ required "Required value 'modelSpec.repository' must be defined !" $modelSpec.repository }}:{{ required "Required value 'modelSpec.tag' must be defined !" $modelSpec.tag }}"
              resources:
                limits:
                  nvidia.com/gpu: {{ .Values.servingEngineSpec.gpuLimit }}
              livenessProbe:
                exec:
                  command: ["/bin/bash", "-c", "echo TBD"]
              readinessProbe:
                exec:
                  command: ["/bin/bash", "-c", "echo TBD"]

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: wait-for-ray-script
data:
  wait_for_ray.py: |
    import ray
    import logging
    import os
    import sys

    logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(message)s')

    try:
        ray.init(address="auto")
    except Exception as e:
        logging.error(f"Failed to initialize Ray: {e}")
        sys.exit(1)

    expected_nodes = int(os.environ.get("EXPECTED_NODES", "1"))

    alive_nodes = [n for n in ray.nodes() if n["Alive"]]
    alive_count = len(alive_nodes)

    logging.info(f"Ray cluster status: {alive_count}/{expected_nodes} nodes alive.")

    if alive_count >= expected_nodes:
        logging.info("Cluster is ready.")
        sys.exit(0)
    else:
        logging.info("Cluster is NOT ready.")
        sys.exit(1)
---

{{- if and $modelSpec.chatTemplate (hasKey $modelSpec "chatTemplateConfigMap") }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: "{{ .Release.Name }}-{{$modelSpec.name}}-chat-templates"
  namespace: "{{ .Release.Namespace }}"
data:
  {{ $modelSpec.chatTemplate }}: |-
    {{ $modelSpec.chatTemplateConfigMap }}
{{- end }}
{{- end }}
---
{{- end }}
{{- end }}
