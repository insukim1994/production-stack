{{- if and .Values.servingEngineSpec.enableEngine .Values.servingEngineSpec.enableKubeRay }}
{{- range $modelSpec := .Values.servingEngineSpec.modelSpec }}
{{- with $ -}}
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: "{{ .Release.Name }}-{{$modelSpec.name}}-raycluster-vllm"
  namespace: {{ .Release.Namespace }}
  labels:
    helm-release-name: {{ .Release.Name }}
spec:
  headGroupSpec:
    serviceType: ClusterIP
    rayStartParams:
      dashboard-host: "0.0.0.0"
    template:
      metadata:
        labels:
          model: {{ $modelSpec.name }}
          {{- include "chart.engineLabels" . | nindent 10 }}
      spec:
        terminationGracePeriodSeconds: 0
        {{- if .Values.servingEngineSpec.securityContext }}
        securityContext:
          {{- toYaml .Values.servingEngineSpec.securityContext | nindent 10 }}
        {{- end }}
        containers:
          - name: vllm-ray-head
            image: "{{ required "Required value 'modelSpec.repository' must be defined !" $modelSpec.repository }}:{{ required "Required value 'modelSpec.tag' must be defined !" $modelSpec.tag }}"
            command:
              - >-
                /bin/bash -c "
                cp /entrypoint/vllm-entrypoint.sh \$HOME/vllm-entrypoint.sh &&
                chmod +x \$HOME/vllm-entrypoint.sh &&
                \$HOME/vllm-entrypoint.sh &
                echo \"Running vllm command in the background.\""
            livenessProbe:
              exec:
                command: ["/bin/bash", "-c", "echo TBD"]
            readinessProbe:
              exec:
                command: ["/bin/bash", "-c", "echo TBD"]
            env:
              - name: EXPECTED_NODES
                value: "{{ add $modelSpec.replicaCount 1}}"
            startupProbe:
              exec:
                command: ["/bin/bash", "-c", "python3 /scripts/wait_for_ray.py"]
              failureThreshold: 30
              periodSeconds: 15
              timeoutSeconds: 10
            volumeMounts:
              - name: wait-script
                mountPath: /scripts
              - name: vllm-script
                mountPath: /entrypoint
        volumes:
          - name: wait-script
            configMap:
              name: wait-for-ray-script
          - name: vllm-script
            configMap:
              name: vllm-start-script
  workerGroupSpecs:
    - rayStartParams: {}
      replicas: {{ $modelSpec.replicaCount }}
      groupName: vllm-ray-worker
      template:
        {{- if .Values.servingEngineSpec.securityContext }}
        securityContext:
          {{- toYaml .Values.servingEngineSpec.securityContext | nindent 8 }}
        {{- end }}
        spec:
          containers:
            - name: vllm-ray-worker
              image: "{{ required "Required value 'modelSpec.repository' must be defined !" $modelSpec.repository }}:{{ required "Required value 'modelSpec.tag' must be defined !" $modelSpec.tag }}"
              resources:
                limits:
                  nvidia.com/gpu: {{ .Values.servingEngineSpec.gpuLimit }}
              livenessProbe:
                exec:
                  command: ["/bin/bash", "-c", "echo TBD"]
              readinessProbe:
                exec:
                  command: ["/bin/bash", "-c", "echo TBD"]

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: wait-for-ray-script
data:
  wait_for_ray.py: |
    import ray
    import logging
    import os
    import sys

    logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(message)s')

    try:
        ray.init(address="auto")
    except Exception as e:
        logging.error(f"Failed to initialize Ray: {e}")
        sys.exit(1)

    expected_nodes = int(os.environ.get("EXPECTED_NODES", "1"))

    alive_nodes = [n for n in ray.nodes() if n["Alive"]]
    alive_count = len(alive_nodes)

    logging.info(f"Ray cluster status: {alive_count}/{expected_nodes} nodes alive.")

    if alive_count >= expected_nodes:
        logging.info("Cluster is ready.")
        sys.exit(0)
    else:
        logging.info("Cluster is NOT ready.")
        sys.exit(1)
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-start-script
data:
  vllm-entrypoint.sh: |
    #!/bin/bash
    set -e

    echo "Waiting for Ray to become available..."
    until python3 /scripts/wait_for_ray.py; do
      echo "Ray not ready yet. Retrying in 2 seconds..."
      sleep 2
    done

    echo "Ray is ready. Starting vLLM..."

    # Start constructing command
    ARGS=(
      "vllm"
      "serve"
      "{{ $modelSpec.modelURL | quote }}"
      "--host" "0.0.0.0"
      "--port" "{{ include "chart.container-port" . }}"
      "--distributed-executor-backend" "ray"
    )

    {{- if $modelSpec.enableLoRA }}
    ARGS+=("--enable-lora")
    {{- end }}

    {{- if $modelSpec.enableTool }}
    ARGS+=("--enable-auto-tool-choice")
    {{- end }}

    {{- if $modelSpec.toolCallParser }}
    ARGS+=("--tool-call-parser" {{ $modelSpec.toolCallParser | quote }})
    {{- end }}

    {{- with $modelSpec.vllmConfig }}
      {{- if hasKey . "enableChunkedPrefill" }}
        {{- if .enableChunkedPrefill }}
    ARGS+=("--enable-chunked-prefill")
        {{- else }}
    ARGS+=("--no-enable-chunked-prefill")
        {{- end }}
      {{- end }}

      {{- if .enablePrefixCaching }}
    ARGS+=("--enable-prefix-caching")
      {{- end }}

      {{- if hasKey . "maxModelLen" }}
    ARGS+=("--max-model-len" {{ .maxModelLen | quote }})
      {{- end }}

      {{- if hasKey . "dtype" }}
    ARGS+=("--dtype" {{ .dtype | quote }})
      {{- end }}

      {{- if hasKey . "tensorParallelSize" }}
    ARGS+=("--tensor-parallel-size" {{ .tensorParallelSize | quote }})
      {{- end }}

      {{- if hasKey . "pipelineParallelSize" }}
    ARGS+=("--pipeline-parallel-size" {{ .pipelineParallelSize | quote }})
      {{- end }}

      {{- if hasKey . "maxNumSeqs" }}
    ARGS+=("--max-num-seqs" {{ .maxNumSeqs | quote }})
      {{- end }}

      {{- if hasKey . "gpuMemoryUtilization" }}
    ARGS+=("--gpu-memory-utilization" {{ .gpuMemoryUtilization | quote }})
      {{- end }}

      {{- if hasKey . "maxLoras" }}
    ARGS+=("--max-loras" {{ .maxLoras | quote }})
      {{- end }}

      {{- range .extraArgs }}
    ARGS+=({{ . | quote }})
      {{- end }}
    {{- end }}

    {{- if $modelSpec.lmcacheConfig }}
      {{- if $modelSpec.lmcacheConfig.enabled }}
        {{- if hasKey $modelSpec.vllmConfig "v1" }}
          {{- if eq (toString $modelSpec.vllmConfig.v1) "1" }}
    ARGS+=("--kv-transfer-config" "{\"kv_connector\":\"LMCacheConnectorV1\",\"kv_role\":\"kv_both\"}")
          {{- else }}
    ARGS+=("--kv-transfer-config" "{\"kv_connector\":\"LMCacheConnector\",\"kv_role\":\"kv_both\"}")
          {{- end }}
        {{- else }}
    ARGS+=("--kv-transfer-config" "{\"kv_connector\":\"LMCacheConnector\",\"kv_role\":\"kv_both\"}")
        {{- end }}
      {{- end }}
    {{- end }}

    {{- if $modelSpec.chatTemplate }}
    ARGS+=("--chat-template" {{ $modelSpec.chatTemplate | quote }})
    {{- end }}

    echo "Executing: ${ARGS[@]}"
    exec "${ARGS[@]}"


{{- if and $modelSpec.chatTemplate (hasKey $modelSpec "chatTemplateConfigMap") }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: "{{ .Release.Name }}-{{$modelSpec.name}}-chat-templates"
  namespace: "{{ .Release.Namespace }}"
data:
  {{ $modelSpec.chatTemplate }}: |-
    {{ $modelSpec.chatTemplateConfigMap }}
{{- end }}
{{- end }}
---
{{- end }}
{{- end }}
